global:
  metadata:
    name: "{{ .ClusterName }}"
    description: "E2E Test cluster"
    organization: "{{ .Organization }}"

  connectivity:
    baseDomain: gaws.gigantic.io
    availabilityZoneUsageLimit: 3
    dns:
      mode: public
    network:
      vpcCidr: 10.{{ index .ExtraValues "CIDRSecondOctet" }}.0.0/18
    proxy:
      enabled: true
      httpProxy: http://proxy.goatproxy.gaws.gigantic.io:4000
      httpsProxy: http://proxy.goatproxy.gaws.gigantic.io:4000
    subnets:
      - cidrBlocks:
          - availabilityZone: a
            cidr: 10.{{ index .ExtraValues "CIDRSecondOctet" }}.0.0/20
          - availabilityZone: b
            cidr: 10.{{ index .ExtraValues "CIDRSecondOctet" }}.16.0/20
          - availabilityZone: c
            cidr: 10.{{ index .ExtraValues "CIDRSecondOctet" }}.32.0/20
        isPublic: false
        tags:
          subnet.giantswarm.io/api-server-elb: 'true'
          subnet.giantswarm.io/bastion: 'true'
          subnet.giantswarm.io/control-plane: 'true'
          subnet.giantswarm.io/endpoints: 'true'
          subnet.giantswarm.io/ingress: 'true'
          subnet.giantswarm.io/tgw: 'true'
          subnet.giantswarm.io/workers: 'true'
    topology:
      mode: GiantSwarmManaged
    vpcMode: private

  controlPlane:
    containerdVolumeSizeGB: 15
    etcdVolumeSizeGB: 50
    kubeletVolumeSizeGB: 10
    rootVolumeSizeGB: 10
    apiMode: private

  # We need to pass the node pools otherwise the test called "has all the worker nodes running" will fail because it
  # expects to find the number of worker nodes in the helm values, but the value is not there if don't pass it, as it's
  # defaulted in the chart template.
  # @TODO: https://github.com/giantswarm/giantswarm/issues/28063
  nodePools:
    # We are using a name with 10 chars which is the max number of characters allowed by our kyverno policies.
    nodepool-0:
      maxSize: 5
      minSize: 2
      rootVolumeSizeGB: 25
      spotInstances:
        enabled: true
        maxPrice: 0.2960

